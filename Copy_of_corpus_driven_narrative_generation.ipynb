{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKKWZM75IJK4"
      },
      "source": [
        "# A few simple corpus-driven approaches to narrative analysis and generation\n",
        "\n",
        "By [Allison Parrish](http://www.decontextualize.com/)\n",
        "\n",
        "This notebook is a fast introduction to a few techniques for working with narrative corpora. By \"narrative corpora,\" I mean pre-existing bodies of text that mostly contain the texts of narratives. In particular, we're going to use Mark Riedl's [WikiPlots corpus](https://github.com/markriedl/WikiPlots), which has the titles and plot summaries of more than one hundred thousand movies, books, television shows and other media from Wikipedia.\n",
        "\n",
        "The notebook takes you through using [spaCy](http://spacy.io) to extract words, noun chunks, parts of speech and entities from the text and then sew them back together with [Tracery](http://tracery.io). It then shows how to use [Markovify](https://github.com/jsvine/markovify) to create new narratives from existing narrative text, and how to prepare the narratives for use as a training corpus for a large pre-trained language model like GPT-2.\n",
        "\n",
        "The code is written in Python, but you don't really need to know Python in order to use the notebook. Everything's pre-written for you, so you can just execute the cells, making small changes to the code as needed. Even if the notebook itself doesn't end up being useful to you, hopefully it spurs a few ideas that you can take with you into your practice as a storyteller and/or programmer.\n",
        "\n",
        "If you're running this code on Binder, you should be good to go. Just keep on executing the cells below. If you're running this notebook on Google Colab, you'll need to run the following cells to install the necessary libraries and download the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ecH54joIJK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ba7f79-0efc-4f5f-c727-e1c120dc7686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting markovify\n",
            "  Downloading markovify-0.9.4.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: markovify\n",
            "  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markovify: filename=markovify-0.9.4-py3-none-any.whl size=18625 sha256=41b21ec2d9e3ab9b2df3efc93a9a5168a47561e419b98fc40640d69735c1400b\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/0a/ab/8727d219981e57e6036316dd2ec2037e61ccea0c016f7ae0c1\n",
            "Successfully built markovify\n",
            "Installing collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.9.4 unidecode-1.3.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tracery\n",
            "  Downloading tracery-0.1.1.tar.gz (8.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tracery\n",
            "  Building wheel for tracery (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tracery: filename=tracery-0.1.1-py3-none-any.whl size=7696 sha256=cfa723b04d95e3562076dce3e5701de1250db36c94415217a201a2021823bdc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/74/46/553a949119086ada4c0ecf608890fb79513349b9057e514885\n",
            "Successfully built tracery\n",
            "Installing collected packages: tracery\n",
            "Successfully installed tracery-0.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.3.2\n",
            "  Downloading spacy-2.3.2.tar.gz (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy==2.3.2) (4.65.0)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Using cached catalogue-1.0.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Using cached blis-0.4.1-cp39-cp39-linux_x86_64.whl\n",
            "Collecting wasabi<1.1.0,>=0.4.0\n",
            "  Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy==2.3.2) (63.4.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy==2.3.2) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy==2.3.2) (2.27.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy==2.3.2) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy==2.3.2) (1.22.4)\n",
            "Collecting thinc==7.4.1\n",
            "  Using cached thinc-7.4.1-cp39-cp39-linux_x86_64.whl\n",
            "Collecting srsly<1.1.0,>=1.0.2\n",
            "  Using cached srsly-1.0.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy==2.3.2) (1.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.3.2) (1.26.15)\n",
            "Building wheels for collected packages: spacy\n",
            "  Building wheel for spacy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy: filename=spacy-2.3.2-cp39-cp39-linux_x86_64.whl size=28801017 sha256=b6d5cc5264a6e741239366672ef4a23ee9a7972728c28572ab33f0eb8f7c5a61\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/e4/3c/6285ea2c318e7dbce4c7fc834228e8e655b51d592e298f4c6e\n",
            "Successfully built spacy\n",
            "Installing collected packages: wasabi, plac, srsly, catalogue, blis, thinc, spacy\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.1\n",
            "    Uninstalling wasabi-1.1.1:\n",
            "      Successfully uninstalled wasabi-1.1.1\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.6\n",
            "    Uninstalling srsly-2.4.6:\n",
            "      Successfully uninstalled srsly-2.4.6\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.9\n",
            "    Uninstalling blis-0.7.9:\n",
            "      Successfully uninstalled blis-0.7.9\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.9\n",
            "    Uninstalling thinc-8.1.9:\n",
            "      Successfully uninstalled thinc-8.1.9\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.1\n",
            "    Uninstalling spacy-3.5.1:\n",
            "      Successfully uninstalled spacy-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 2.3.2 which is incompatible.\n",
            "confection 0.0.4 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-0.4.1 catalogue-1.0.2 plac-1.1.3 spacy-2.3.2 srsly-1.0.6 thinc-7.4.1 wasabi-0.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_sm==2.3.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m140.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.9/dist-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.27.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.22.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.9/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.6)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.9/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (63.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.65.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.7)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.9/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.10.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4)\n",
            "Building wheels for collected packages: en_core_web_sm\n",
            "  Building wheel for en_core_web_sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en_core_web_sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047102 sha256=be51bfafa3d637a5930b250c4325cdfd92a1ed77f602ad46b1cb788c918b2c22\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rs59q30n/wheels/19/d6/1c/5484b95647df5d7afaf74abde458c66c1cd427e69e801fe826\n",
            "Successfully built en_core_web_sm\n",
            "Installing collected packages: en_core_web_sm\n",
            "  Attempting uninstall: en_core_web_sm\n",
            "    Found existing installation: en-core-web-sm 3.5.0\n",
            "    Uninstalling en-core-web-sm-3.5.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.5.0\n",
            "Successfully installed en_core_web_sm-2.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 4014k  100 4014k    0     0  3593k      0  0:00:01  0:00:01 --:--:-- 3593k\n"
          ]
        }
      ],
      "source": [
        "!pip install markovify\n",
        "!pip install tracery\n",
        "!pip install spacy==2.3.2\n",
        "!python -m spacy download en_core_web_sm\n",
        "!curl -L -O https://github.com/aparrish/corpus-driven-narrative-generation/raw/master/romcom_plot_sentences.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mUL0IeoIJK9"
      },
      "source": [
        "## Loading the corpus\n",
        "\n",
        "The first step is to get the narrative corpus into the program. Because WikiPlots is so big, we're actually going to be working with a smaller subset: only the plot summaries for romantic comedy movies. The subcorpus was made using [this notebook on creating a subcorpus of WikiPlots](https://github.com/aparrish/corpus-driven-narrative-generation/blob/master/creating-a-wikiplots-subcorpus.ipynb), which you can consult if you want to make your own with a different subset of WikiPlots.\n",
        "\n",
        "The corpus we're working with takes the form of a TSV file (\"tab separated values\"), with each line containing the title of the movie, a number indicating where in the plot summary the sentence for this line occurs, the total number of sentences in the summary, and the actual text of the sentence. The following cell loads the data into a list of dictionaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "fm_OK3ZTIJK-"
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "for line in open(\"romcom_plot_sentences.tsv\"):\n",
        "    line = line.strip()\n",
        "    items = line.split(\"\\t\")\n",
        "    sentences.append(\n",
        "        {'title': items[0],\n",
        "         'index': int(items[1]),\n",
        "         'total': int(items[2]),\n",
        "         'text': items[3]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4-ky3gUIJK-"
      },
      "source": [
        "Just to make sure it worked, we'll print out a random sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFIsdfTnIJK-"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PH3n5xfIJK_",
        "outputId": "240ed696-9959-4f37-9973-1363dbb319b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Fools for Scandal',\n",
              " 'index': 20,\n",
              " 'total': 35,\n",
              " 'text': 'After tasting the food that Rene prepares, Kay, as a joke, offers him a job as her cook.'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "random.choice(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgyWI4fVIJK_"
      },
      "source": [
        "Note: You can make your own corpus that works with the code in this notebook by exporting your data in TSV format with one line per sentence, with columns for the following:\n",
        "\n",
        "* `title`: the title of the work that the sentence comes from\n",
        "* `index`: the index of the sentence in the work\n",
        "* `total`: the total number of sentences in the work\n",
        "* `text`: the text of the sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl5iDBihIJLA"
      },
      "source": [
        "## Natural language processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwjfkSefIJLA"
      },
      "source": [
        "To get an idea of what's happening in the text of the plots, we can do a bit of Natural Language Processing. I cover just the bare essentials in this notebook. [Here's a more in-depth tutorial that I wrote](https://github.com/aparrish/rwet/blob/master/nlp-concepts-with-spacy.ipynb).\n",
        "\n",
        "Most natural language processing is done with the aid of third-party libraries. We're going to use one called spaCy. To use spaCy, you first need to install it (i.e., download the code and put it in a place where Python can find it) and download the language model. (The language model contains statistical information about a particular language that makes it possible for spaCy to do things like parse sentences into their constituent parts.)\n",
        "\n",
        "Run the following cell to load spaCy's model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-rtPZjvIJLA"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPOrV5JKIJLB"
      },
      "source": [
        "(This could also take a while–the model is potentially very large and your computer needs to load it from your hard drive and into memory. When you see a `[*]` next to a cell, that means that your computer is still working on executing the code in the cell.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_iRoV6OIJLB"
      },
      "source": [
        "Right off the bat, the spaCy library gives us access to a number of interesting units of text:\n",
        "\n",
        "* All of the sentences (`doc.sents`)\n",
        "* All of the words (`doc`)\n",
        "* All of the \"named entities,\" like names of places, people, #brands, etc. (`doc.ents`)\n",
        "* All of the \"noun chunks,\" i.e., nouns in the text plus surrounding matter like adjectives and articles\n",
        "\n",
        "The cell below, we extract these into variables so we can play around with them a little bit. (Parsing sentences is hungry work and the following cell will take a while to execute.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-BUluDvIJLC",
        "outputId": "4a842a65-8a63-4541-996c-955c5e6c1ea8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 28785\n",
            "100 28785\n",
            "200 28785\n",
            "300 28785\n",
            "400 28785\n",
            "500 28785\n",
            "600 28785\n",
            "700 28785\n",
            "800 28785\n",
            "900 28785\n"
          ]
        }
      ],
      "source": [
        "words = []\n",
        "noun_chunks = []\n",
        "entities = []\n",
        "# only use 1000 sentences sampled at random by default; comment out this `for...`\n",
        "# uncomment the `for...` beneath to use every sentence in the corpus.\n",
        "for i, sent in enumerate(random.sample(sentences, 1000)):\n",
        "#for i, sent in enumerate(sentences):\n",
        "    if i % 100 == 0:\n",
        "        print(i, len(sentences))\n",
        "    doc = nlp(sent['text'])\n",
        "    words.extend([w for w in list(doc) if w.is_alpha])\n",
        "    noun_chunks.extend(list(doc.noun_chunks))\n",
        "    entities.extend(list(doc.ents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_os2gs2IJLC"
      },
      "source": [
        "Just to make sure it worked, print out ten random words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qnhAeYgIJLC",
        "outputId": "b460dad4-e672-4df0-97fe-74fe8dc6fe87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "asks\n",
            "They\n",
            "and\n",
            "to\n",
            "the\n",
            "home\n",
            "control\n",
            "They\n",
            "home\n",
            "is\n"
          ]
        }
      ],
      "source": [
        "for item in random.sample(words, 10):\n",
        "    print(item.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td6j2-s5IJLC"
      },
      "source": [
        "Ten random noun chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYqCSs07IJLD",
        "outputId": "0d6b166e-4bc6-4cb7-9d53-3020b42a75d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Ghost\n",
            "the rest\n",
            "Ken Leung\n",
            "she\n",
            "she\n",
            "Judson\n",
            "particularly rebellious teenagers\n",
            "Housing\n",
            "family\n",
            "Albert Ebbs\n"
          ]
        }
      ],
      "source": [
        "for item in random.sample(noun_chunks, 10):\n",
        "    print(item.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cySksQZRIJLD"
      },
      "source": [
        "Ten random entities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJDwYLyNIJLD",
        "outputId": "080d82b7-ecca-4faa-e42c-0f11d574c068",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "David\n",
            "Holly\n",
            "the next morning\n",
            "Cam\n",
            "Matt\n",
            "the Department of Housing and Building\n",
            "David\n",
            "Mark\n",
            "Nina\n",
            "Charlie Chaplin\n"
          ]
        }
      ],
      "source": [
        "for item in random.sample(entities, 10):\n",
        "    print(item.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp2_b-smIJLD"
      },
      "source": [
        "### Grammatical roles\n",
        "\n",
        "The parser included with spaCy can also give us information about the grammatical roles in the sentence. For example, the `.root.dep_` attribute of a noun chunk tells us whether that noun chunk is the subject of the sentence (\"nsubj\") or a direct object (\"dobj\") of the sentence. (See the \"Universal Dependency Labels\" of spaCy's [annotation specs](https://spacy.io/api/annotation) for more possible roles.) Using this information, we can make a list of sentence subjects and sentence objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ug2a9ea_IJLD"
      },
      "outputs": [],
      "source": [
        "subjects = [chunk for chunk in noun_chunks if chunk.root.dep_ == 'nsubj']\n",
        "objects = [chunk for chunk in noun_chunks if chunk.root.dep_ == 'dobj']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNK28qmFIJLE",
        "outputId": "43034b63-b4a6-4896-eb80-be4c37e00130",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Julie,\n",
              " Debbie,\n",
              " Marissa,\n",
              " life,\n",
              " Jon,\n",
              " Melodie,\n",
              " she,\n",
              " Fella's (Jerry Lewis) father,\n",
              " Samantha,\n",
              " Vice Squad Sergeant Sam Hanlon]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "random.sample(subjects, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBOeLqKeIJLE",
        "outputId": "ff37392f-f15f-445c-b7fb-29c84ac1e8e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[him,\n",
              " a letter,\n",
              " a newspaper article,\n",
              " things,\n",
              " whose baby,\n",
              " her popularity,\n",
              " Val,\n",
              " Richard,\n",
              " time,\n",
              " VIP privileges]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "random.sample(objects, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGf6GJtLIJLE"
      },
      "source": [
        "### Parts of speech\n",
        "\n",
        "The spaCy parser allows us to check what part of speech a word belongs to. In the cell below, we create four different lists—`nouns`, `verbs`, `adjs` and `advs`—that contain only words of the specified parts of speech. Using the `.tag_` attribute, we can easily get only particular forms of verbs; in this case, I'm just getting verbs that are in the past tense. ([There's a full list of part of speech tags here](https://spacy.io/docs/usage/pos-tagging#pos-tagging-english).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChK3_sQtIJLE"
      },
      "outputs": [],
      "source": [
        "nouns = [w for w in words if w.pos_ == \"NOUN\"]\n",
        "verbs = [w for w in words if w.pos_ == \"VERB\"]\n",
        "past_tense_verbs = [w for w in words if w.tag_ == 'VBD']\n",
        "adjs = [w for w in words if w.tag_ == \"JJ\"]\n",
        "advs = [w for w in words if w.pos_ == \"ADV\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AHAT2t-IJLE"
      },
      "source": [
        "And now we can print out a random sample of any of these:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vk95TFgsIJLF",
        "outputId": "d3f14815-2771-4e1c-91f0-abc935ab1d2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "life\n",
            "engagement\n",
            "Blood\n",
            "day\n",
            "Kelp\n",
            "task\n",
            "wife\n",
            "appearances\n",
            "head\n",
            "confessionals\n",
            "beginning\n",
            "thread\n"
          ]
        }
      ],
      "source": [
        "for item in random.sample(nouns, 12): # change \"nouns\" to \"verbs\" or \"adjs\" or \"advs\" to sample from those lists!\n",
        "    print(item.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_0j5Jx3IJLF"
      },
      "source": [
        "### Entity types\n",
        "\n",
        "The parser in spaCy not only identifies \"entities\" but also assigns them to a particular type. [See a full list of entity types here.](https://spacy.io/docs/usage/entity-recognition#entity-types) Using this information, the following cell builds lists of the people, locations, and times mentioned in the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUgjQfgoIJLF"
      },
      "outputs": [],
      "source": [
        "people = [e for e in entities if e.label_ == \"PERSON\"]\n",
        "locations = [e for e in entities if e.label_ == \"LOC\"]\n",
        "times = [e for e in entities if e.label_ == \"TIME\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMXZ_hrtIJLF"
      },
      "source": [
        "And then you can print out a random sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guu098JTIJLF",
        "outputId": "3b05e5c9-02d0-4585-f262-a05d90475e63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One morning\n",
            "hours\n",
            "the previous night\n",
            "the next morning\n",
            "Later that night\n",
            "one night\n",
            "night\n",
            "a night\n",
            "That evening\n",
            "the night\n",
            "night\n",
            "that night\n"
          ]
        }
      ],
      "source": [
        "for item in random.sample(times, 12): # change \"times\" to \"people\" or \"locations\" to sample those lists\n",
        "    print(item.text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuSWFWgZIJLF"
      },
      "source": [
        "### Finding the most common\n",
        "\n",
        "We won't go too deep into text analysis in this tutorial, but it's useful to be able to do the most fundamental task in text analysis: finding the things that are most common. The code to do this task looks like the following, which gives us a way to look up how often any word occurs in the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1-ra6ClIJLF"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "word_count = Counter([w.text for w in words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPBT-dJBIJLG",
        "outputId": "2ca57a79-2a5b-44a5-f8e3-ae7be06dd167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "word_count['Meanwhile']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYZhHmpzIJLG"
      },
      "source": [
        "... and also tells us which words are most common:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHstj5JjIJLG",
        "outputId": "1a085024-d92f-4b49-81b1-546d80fcaee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 917),\n",
              " ('to', 784),\n",
              " ('and', 719),\n",
              " ('a', 560),\n",
              " ('her', 375),\n",
              " ('is', 364),\n",
              " ('of', 320),\n",
              " ('in', 296),\n",
              " ('his', 289),\n",
              " ('with', 288),\n",
              " ('that', 251),\n",
              " ('he', 247)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "word_count.most_common(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcuMP0qUIJLG"
      },
      "source": [
        "You can make a counter for any of the other lists we've worked with using the same syntax. Just make up a unique variable name on the left of the `=` sign and put the name of the list you want to count in the brackets to the right (replacing `words`). E.g., to find the most common people:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrdNepKuIJLG"
      },
      "outputs": [],
      "source": [
        "people_count = Counter([w.text for w in people])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ZMZtbHTiIJLG",
        "outputId": "d28ebaba-6432-4692-94f9-d59841ef67e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Tom', 20),\n",
              " ('Mary', 12),\n",
              " ('George', 11),\n",
              " ('Joe', 11),\n",
              " ('Julie', 10),\n",
              " ('Sam', 10),\n",
              " ('Jeff', 9),\n",
              " ('Sally', 9),\n",
              " ('Kate', 9),\n",
              " ('Anna', 9),\n",
              " ('Peter', 9),\n",
              " ('Ben', 9)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "people_count.most_common(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZqUbqzaIJLG"
      },
      "source": [
        "The most common past-tense verbs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f_j0WLIIJLH"
      },
      "outputs": [],
      "source": [
        "vbd_count = Counter([w.text for w in past_tense_verbs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vERR4fyeIJLH",
        "outputId": "25101e6c-e118-45d7-ba1e-dba342a50f44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('was', 30),\n",
              " ('had', 28),\n",
              " ('left', 6),\n",
              " ('did', 6),\n",
              " ('set', 5),\n",
              " ('were', 4),\n",
              " ('became', 4),\n",
              " ('took', 3),\n",
              " ('wrote', 3),\n",
              " ('gave', 3),\n",
              " ('thought', 3),\n",
              " ('went', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "vbd_count.most_common(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9Rw5_3WIJLH"
      },
      "source": [
        "### Writing to a file\n",
        "\n",
        "The following cell defines a function for writing data from a `Counter` object to a file. The file is in \"tab-separated values\" format, which you can open using most spreadsheet programs. Execute it before you continue:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5AHjJ6ZIJLH"
      },
      "outputs": [],
      "source": [
        "def save_counter_tsv(filename, counter, limit=1000):\n",
        "    with open(filename, \"w\") as outfile:\n",
        "        outfile.write(\"key\\tvalue\\n\")\n",
        "        for item, count in counter.most_common():\n",
        "            outfile.write(item.strip() + \"\\t\" + str(count) + \"\\n\")    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg3J9q-uIJLH"
      },
      "source": [
        "Now, run the following cell. You'll end up with a file in the same directory as this notebook called `100_common_words.tsv` that has two columns, one for the words and one for their associated counts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C20puqHvIJLH"
      },
      "outputs": [],
      "source": [
        "save_counter_tsv(\"100_common_words.tsv\", word_count, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlrmMJGwIJLH"
      },
      "source": [
        "Try opening this file in Excel or Google Docs or Numbers!\n",
        "\n",
        "If you want to write the data from another `Counter` object to a file:\n",
        "\n",
        "* Change the filename to whatever you want (though you should probably keep the `.tsv` extension)\n",
        "* Replace `word_count` with the name of any of the `Counter` objects we've made in this sheet and use it in place of `word_count`\n",
        "* Change the number to the number of rows you want to include in your spreadsheet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYz0Sk7iIJLH"
      },
      "source": [
        "### When do things happen in this text?\n",
        "\n",
        "Here's another example. Using the `times` entities, we can make a spreadsheet of how often particular \"times\" (durations, times of day, etc.) are mentioned in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW3MF23PIJLI"
      },
      "outputs": [],
      "source": [
        "time_counter = Counter([e.text.lower().strip() for e in times])\n",
        "save_counter_tsv(\"time_count.tsv\", time_counter, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLbltW1JIJLI"
      },
      "source": [
        "Do the same thing, but with people:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnZ8OiANIJLI"
      },
      "outputs": [],
      "source": [
        "people_counter = Counter([e.text.lower() for e in people])\n",
        "save_counter_tsv(\"people_count.tsv\", people_counter, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDbeEJ0yIJLI"
      },
      "source": [
        "### Generating stories from a corpus and Tracery grammars\n",
        "\n",
        "Once you've isolated entities and parts of speech, you can recombine them in interesting ways. One is to use a Tracery grammar to write sentences that include the isolated parts. Because the parts have been labelled using spaCy, you can be reasonbly sure that they'll fit into particular slots in the sentence. (I used a similar technique for my [Cheap Space Nine](https://twitter.com/cheapspacenine) bot.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvWh9tJDIJLI"
      },
      "outputs": [],
      "source": [
        "import tracery\n",
        "from tracery.modifiers import base_english"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTPNZVJzIJLI"
      },
      "outputs": [],
      "source": [
        "rules = {\n",
        "    \"subject\": [w.text for w in subjects],\n",
        "    \"object\": [w.text for w in objects],\n",
        "    \"verb\": [w.text for w in past_tense_verbs if w.text not in ('was', 'were', 'went')], # exclude common irregular verbs\n",
        "    \"adj\": [w.text for w in adjs],\n",
        "    \"people\": [w.text for w in people],\n",
        "    \"loc\": [w.text for w in locations],\n",
        "    \"time\": [w.text for w in times],\n",
        "    \"origin\": \"#scene#\\n\\n[charA:#subject#][charB:#subject#][prop:#object#]#sentences#\",\n",
        "    \"scene\": \"SCENE: #loc#, #time.lowercase#\",\n",
        "    \"sentences\": [\n",
        "        \"#sentence#\\n#sentence#\",\n",
        "        \"#sentence#\\n#sentence#\\n#sentence#\",\n",
        "        \"#sentence#\\n#sentence#\\n#sentence#\\n#sentence#\"\n",
        "    ],\n",
        "    \"sentence\": [\n",
        "        \"#charA.capitalize# #verb# #prop#.\",\n",
        "        \"#charB.capitalize# #verb# #prop#.\",\n",
        "        \"#prop.capitalize# became #adj#.\",\n",
        "        \"#charA.capitalize# and #charB# greeted each other.\",\n",
        "        \"'Did you hear about #object.lowercase#?' said #charA#.\",\n",
        "        \"'#subject.capitalize# is #adj#,' said #charB#.\",\n",
        "        \"#charA.capitalize# and #charB# #verb# #object#.\",\n",
        "        \"#charA.capitalize# and #charB# looked at each other.\",\n",
        "        \"#sentence#\\n#sentence#\"\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rmg2P1AJIJLI"
      },
      "outputs": [],
      "source": [
        "grammar = tracery.Grammar(rules)\n",
        "grammar.add_modifiers(base_english)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAxOm-0qIJLI",
        "outputId": "3181e1e5-bd80-455a-fa43-bbd2727602b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SCENE: Botts, morning\n",
            "\n",
            "He and he presented more independence.\n",
            "He and he greeted each other.\n",
            "He became his makeup.\n",
            "He and he had Debi.\n",
            "\n",
            "SCENE: Olivia, one morning\n",
            "\n",
            "'Did you hear about her?' said They.\n",
            "They and She pronounced their \"disgusting three-sided erotic hotch-potch.\n",
            "'Did you hear about a limited amount?' said They.\n",
            "\n",
            "SCENE: Delta Gamma, later that night\n",
            "\n",
            "Conrado became same.\n",
            "'Semmi is ninth,' said the water.\n",
            "Conrado became penniless.\n",
            "Conrado became pregnant.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    print(grammar.flatten(\"#origin#\"))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74DRvlNoIJLJ"
      },
      "source": [
        "## Markov chain text generation\n",
        "\n",
        "Another way to produce new narratives from existing narrative text is to find statistical patterns in the text itself and then make the computer create new text that follows those statistical patterns. Markov chain text generation has been a pastime of poets and programmers going back [all the way to 1983](https://www.jstor.org/stable/24969024), so it should be no surprise that there are many implementations of the idea in Python that you can download and install. The one we're going to use is [Markovify](https://github.com/jsvine/markovify), a Markov chain text generation library originally developed for BuzzFeed, apparently. Writing [code to implement a Markov chain generator](https://github.com/aparrish/rwet/blob/master/ngrams-and-markov-chains.ipynb) on your own is certainly possible, but Markovify comes with a lot of extra niceties that will make our lives easier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE9pCMSRIJLJ"
      },
      "source": [
        "To install Markovify on your computer, run the cell below. (You can skip this step if you're using this notebook in Binder.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "mQu3Imp5IJLJ",
        "outputId": "731f30ae-611b-4042-90f8-3d0fb1306801",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: markovify in /usr/local/lib/python3.9/dist-packages (0.9.4)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.9/dist-packages (from markovify) (1.3.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install markovify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXUW0fuYIJLJ"
      },
      "source": [
        "And then run this cell to make the library available in your notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "NRyLjSYAIJLJ"
      },
      "outputs": [],
      "source": [
        "import markovify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9vIGWVSIJLJ"
      },
      "source": [
        "We need a list of strings to train the Markov generator. For now, let's just get all of the sentences from any movie in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_-PHdbkIJLJ"
      },
      "outputs": [],
      "source": [
        "all_text = [item['text'] for item in sentences]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV3SyxNrIJLK"
      },
      "source": [
        "The code in the following cell creates a new text generator, using the text in the variable specified to build the Markov model, which is then assigned to the variable `all_text_gen`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Jj5eu4RtIJLK"
      },
      "outputs": [],
      "source": [
        "all_text_gen = markovify.Text(all_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4l4OkYtIJLK"
      },
      "source": [
        "You can then call the `.make_sentence()` method to generate a sentence from the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "wH6QNpWRIJLK",
        "outputId": "59a5cd71-5ea2-4834-ed57-bf706e6f147b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As Bob consoles Polly, Alysia breaks off her attempts to take off his debt instead of his youth and charm.\n"
          ]
        }
      ],
      "source": [
        "print(all_text_gen.make_sentence())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyek8mXBIJLK"
      },
      "source": [
        "The `.make_short_sentence()` method allows you to specify a maximum length for the generated sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0-fVlKIwIJLK",
        "outputId": "8183630e-8383-435b-c01f-bd5f65667a2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They kiss and make a picture.\n"
          ]
        }
      ],
      "source": [
        "print(all_text_gen.make_short_sentence(50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sAoPeESIJLK"
      },
      "source": [
        "By default, Markovify tries to generate a sentence that is significantly different from any existing sentence in the input text. As a consequence, sometimes the `.make_sentence()` or `.make_short_sentence()` methods will return `None`, which means that in ten tries it wasn't able to generate such a sentence. You can work around this by increasing the number of times it tries to generate a sufficiently unique sentence using the `tries` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_gen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCPdMJ_6OzoF",
        "outputId": "45091c9e-a9ea-47da-9f67-002b950cb627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<markovify.text.Text at 0x7f925fa72700>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "WDZkjDICIJLK",
        "outputId": "873dfe24-2444-4361-c5be-41254bcb3a78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She says that her marriage work.\n"
          ]
        }
      ],
      "source": [
        "print(all_text_gen.make_short_sentence(40, tries=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPBKo8xNIJLK"
      },
      "source": [
        "Or by disabling the check altogether with `test_output=False`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "jJzrfWgCIJLL",
        "outputId": "fbdd4a8a-9b4d-45c1-ad52-6be64a93344d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chelsea goes to hunt her up.\n"
          ]
        }
      ],
      "source": [
        "print(all_text_gen.make_short_sentence(40, test_output=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n9IFue4IJLL"
      },
      "source": [
        "### Changing the order\n",
        "\n",
        "When you create the model, you can specify the order of the model using the `state_size` parameter. It defaults to 2. Let's make two model with different orders and compare:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "E781G0LWIJLL"
      },
      "outputs": [],
      "source": [
        "gen_1 = markovify.Text(all_text, state_size=1)\n",
        "gen_4 = markovify.Text(all_text, state_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "eCmFD_7NIJLL",
        "outputId": "de46c4ca-eb61-49a4-b1cc-27b1f0ef89e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "order 1\n",
            "A relationship with his conquests punches Peter's solution is not get caught about the night while he is, forcing Harper the end, Gloria is in their families are now a rhinoceros, but she reciprocates.\n",
            "\n",
            "order 4\n",
            "She is slightly older and is grateful to receive a proposal that will guarantee her a home.\n"
          ]
        }
      ],
      "source": [
        "print(\"order 1\")\n",
        "print(gen_1.make_sentence(test_output=False))\n",
        "print()\n",
        "print(\"order 4\")\n",
        "print(gen_4.make_sentence(test_output=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh4XKygZIJLL"
      },
      "source": [
        "In general, the higher the order, the more the sentences will seem \"coherent\" (i.e., more closely resembling the source text). Lower order models will produce more variation. Deciding on the order is usually a matter of taste and trial-and-error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N21eiBC_IJLL"
      },
      "source": [
        "### Changing the level\n",
        "\n",
        "Markovify, by default, works with *words* as the individual unit. It doesn't come out-of-the-box with support for character-level models. The following code defines a new kind of Markovify generator that implements character-level models. Execute it before continuing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "SBP9NP0YIJLL"
      },
      "outputs": [],
      "source": [
        "class SentencesByChar(markovify.Text):\n",
        "    def word_split(self, sentence):\n",
        "        return list(sentence)\n",
        "    def word_join(self, words):\n",
        "        return \"\".join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcUlrc61IJLL"
      },
      "source": [
        "Any of the parameters you passed to `markovify.Text` you can also pass to `SentencesByChar`. The `state_size` parameter still controls the order of the model, but now the n-grams are characters, not words.\n",
        "\n",
        "The following cell implements a character-level Markov text generator for the word \"condescendences\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CX4_9ZnBIJLL"
      },
      "outputs": [],
      "source": [
        "con_model = SentencesByChar(\"condescendences\", state_size=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CgfQ7kFIJLM"
      },
      "source": [
        "Execute the cell below to see the output—it'll be a lot like what we implemented by hand earlier!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "MNy1gqhGIJLM",
        "outputId": "3304552a-c6bb-4619-830f-c88447e154ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'condencencencesces'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "con_model.make_sentence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0qB_O0XIJLM"
      },
      "source": [
        "Of course, you can use a character-level model on any text of your choice. So, for example, the following cell creates a character-level order-7 Markov chain text generator from text A:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "NlrXM6WdIJLM"
      },
      "outputs": [],
      "source": [
        "gen_char = SentencesByChar(all_text, state_size=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN3iL3kDIJLM"
      },
      "source": [
        "And the cell below prints out a random sentence from this generator. (The `.replace()` is to get rid of any newline characters in the output.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "2f5oMjGMIJLM",
        "outputId": "649cf265-a01a-4dab-c91c-e42893ad816f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enraged with the men win the procedure by three women Chuck tries to play one man, Slick whacks his surprisingly drink and the pair falls and the curse.\n"
          ]
        }
      ],
      "source": [
        "print(gen_char.make_sentence(test_output=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oauLIx-IJLM"
      },
      "source": [
        "### Thinking about structure\n",
        "\n",
        "It's one thing to be able to produce one plausible sentence of a plot summary using Markov chains, but another to create a sense of overall structure between sentences, and generating narratives with these kinds of long-term dependencies is still an open problem in computational creativity. The approach I'm going to suggest below relies on the intuition that sentences in a plot summary share characteristics based on their position in the summary. First sentences will generally introduce characters and present an initial situation; last sentences will generally describe how the situation was resolved; and sentences in between will describe developing action.\n",
        "\n",
        "Following this intuition, let's create *three different Markov chains*: one for beginning sentences, one for middle sentences, and one for final sentences. We can use the `index` of each sentence in our corpus to give us this information.\n",
        "\n",
        "First, the beginnings are lines whose index is zero (i.e., they're the first sentence for this plot):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VTnQzzfIJLM"
      },
      "outputs": [],
      "source": [
        "beginnings = [line['text'] for line in sentences if line['index'] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYeLe10bIJLM",
        "outputId": "8f8aa722-d3f4-4c7f-bb9d-727f9c99fa1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lambert Hunkins (Frank McHugh) works at a linoleum company.',\n",
              " 'Dustin Willoughby (Joe Brown) is a prizefighter and believer in astrology, who only wins when the stars are in alignment.',\n",
              " \"Students from the suburbs of Philadelphia are attending a high-school graduation party at a large house owned by a rich class member's family.\",\n",
              " 'Mimi Glossop (Ginger Rogers) arrives in England to seek a divorce from her geologist husband Cyril, whom she has not seen for several years.',\n",
              " 'Ann Barton (Laura La Plante), a girl from a once-wealthy family, must make a living by clerking in a cigar store.']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "random.sample(beginnings, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi_uH7TOIJLN"
      },
      "source": [
        "And endings are sentences that come last in the plot (i.e., their index is one less than the total number of sentences):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDICcnooIJLN"
      },
      "outputs": [],
      "source": [
        "endings = [line['text'] for line in sentences if line['index'] == line['total'] - 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3dGrLDHFIJLN",
        "outputId": "c8165fb0-e973-40ad-bc7d-5b78e6ab71dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['While Cristina dates Paulo the owner of the dating agency, Chico, begins to fall in love with Cristina.',\n",
              " 'Joel accepts this, and they decide to attempt a relationship anyway, starting their life together anew.',\n",
              " 'Erika is arrested and sentenced to serve time in a labor camp, and Phoebe and John are reunited.',\n",
              " \"In the end, he not only is able to get his money back and adopt a child, but Linda discovers she's pregnant.\",\n",
              " 'A later newspaper headline states that Governor Hall has decided to run for president.']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "random.sample(endings, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aQomu3BIJLN"
      },
      "source": [
        "And \"middles\" are anything in between:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umV1CFqDIJLN"
      },
      "outputs": [],
      "source": [
        "middles = [line['text'] for line in sentences if 0 < line['index'] < line['total'] - 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3XriRmVIJLN",
        "outputId": "11ac37f6-8d23-425e-8e5b-0a163091faab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Mike wins the bet but he doesn't tell Roland and Slim out of respect for Alicia.\",\n",
              " 'Over time, Nick succeeds to rekindle some of his female acquaintance/ relationships especially at work.',\n",
              " \"Later, Ricardo privately tells Frances' producer that Frances may not be right for the part and that he had a younger actress in mind.\",\n",
              " 'After several months of dating, Grace finally decides to tell Bob about her transplant.',\n",
              " 'They talk to each other and discover that both Bruce and Peter Danilo wanted Eric separated from Polly, for different selfish reasons.']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "random.sample(middles, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8MGBsOqIJLN"
      },
      "source": [
        "The following cell creates the models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ_6VIwFIJLN"
      },
      "outputs": [],
      "source": [
        "beginning_gen = markovify.Text(beginnings)\n",
        "middle_gen = markovify.Text(middles)\n",
        "ending_gen = markovify.Text(endings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WetuE1iTIJLN"
      },
      "source": [
        "Now you can generate tiny narratives by producing a beginning sentence, a middle sentence, and an ending sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3R8ZOs0IJLN",
        "outputId": "f2b1d897-c82a-4588-b6d8-12b59d966346",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corie and Paul Bratter are a very good jazz quartet, but they perform in a museum.\n",
            "An epilogue shows the couple's relationship as a threesome.\n",
            "Chris declares his love for each other.\n"
          ]
        }
      ],
      "source": [
        "print(beginning_gen.make_short_sentence(100))\n",
        "print(middle_gen.make_short_sentence(100))\n",
        "print(ending_gen.make_short_sentence(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvQ3vEX9IJLO"
      },
      "source": [
        "The narratives still feel disconnected (and there are often jarring mismatches in pronoun antecedents), but the artifacts produced with this method do feel a bit narrative-like? Maybe?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aULmyILTIJLO"
      },
      "source": [
        "### Combining models\n",
        "\n",
        "Markovify has a handy feature that allows you to *combine* models, creating a new model that draws on probabilities from both of the source models. You can use this to create hybrid output that mixes the style and content of two (or more!) different source texts. To do this, you need to create the models independently, and then call `.combine()` to combine them.\n",
        "\n",
        "The code below combines models for beginning sentences, middle sentences, and ending sentences into one model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "rIHiu9bmIJLO"
      },
      "outputs": [],
      "source": [
        "combo = markovify.combine([beginning_gen, middle_gen, ending_gen], [10, 1, 10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzbKCYNkIJLO"
      },
      "source": [
        "The bit of code `[10, 1, 10]` controls the \"weights\" of the models, i.e., how much to emphasize the probabilities of any model. You can change this to suit your tastes. (E.g., if you want mostly beginnings with but a bit of middles and a *soupçon* of ends, try `[10, 2, 1]`.)\n",
        "\n",
        "Then you can create sentences using the combined model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "FbGYYtmFIJLO",
        "outputId": "5a927c52-10a7-421d-c7f9-284ff6b96773",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stevie accidentally tips off Jonah that they have a child with her.\n"
          ]
        }
      ],
      "source": [
        "print(combo.make_short_sentence(120))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(combo.make_short_sentence(121))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2ljK03gObF9",
        "outputId": "7b94c26a-b5b9-4e70-b7d3-0548c66696dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom and Gloria who elope.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The .make_short_sentence() method allows you to specify a maximum length for the generated sentence:\n",
        "\n",
        "By default, Markovify tries to generate a sentence that is significantly different from any existing sentence in the input text. As a consequence, sometimes the .make_sentence() or .make_short_sentence() methods will return None, which means that in ten tries it wasn't able to generate such a sentence. You can work around this by increasing the number of times it tries to generate a sufficiently unique sentence using the tries parameter:\n",
        "\n"
      ],
      "metadata": {
        "id": "AvDV7lERPBhT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYftCKMHIJLO"
      },
      "source": [
        "## Prepping the corpus for fine-tuning a large language model\n",
        "\n",
        "Markov chains are cheap and fun, but they don't do a great job of the one thing we expect from stories: maintaining coherence over a long stretch of text. Accomplishing this is a more difficult task, and requires making use of more sophisticated machine learning models, belonging to the category of large pre-trained neural networks. These models are fundamentally similar to Markov chains, in that they make a prediction about what will come next in a text, given some stretch of context. Unlike a Markov chain, a large pre-trained neural network can predict what will come next in a text, even if the context you give it has never been seen in the training text. It can also work on contexts of arbitrary and variable length. Handy!\n",
        "\n",
        "These language models are already trained on a large amount of text. Generally, you don't train them from scratch on your own, but instead \"fine-tune\" them to bring their probabilities more in line with a particular source text.\n",
        "\n",
        "One such model, [OpenAI's GPT-2](https://github.com/openai/gpt-2) does a pretty good job of maintaining long-distance coherence, and it's easy to fine-tune the model with Max Woolf's [aitextgen](https://github.com/minimaxir/aitextgen/).  We'll use the [example Colab notebook](https://colab.research.google.com/drive/15qBZx5y9rdaQSyWpsreMDnTiZ5IlN0zD?usp=sharing) from the aitextgen repository. This notebook works best when it's fine-tuned on text in a prose format. The model can also learn ad-hoc markup elements that you add to the text. We'll use this feature of the model to make it possible to generate stories from beginning to end, by adding a `[BEGIN STORY]` marker before each story in the source text, followed by the title of the story."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASLSq2n4NyYv",
        "outputId": "7eadbbe9-7b57-4ffb-c813-ae2f74fecf78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Four Weddings and a Funeral',\n",
              " 'index': 1,\n",
              " 'total': 36,\n",
              " 'text': 'The first wedding is that of Angus and Laura, at which Charles is the best man.'}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9gh6Uf3IJLO"
      },
      "outputs": [],
      "source": [
        "out = []\n",
        "last_title = None\n",
        "for sent in sentences[:10000]:\n",
        "    if sent['title'] != last_title:\n",
        "        out.append(\"\")\n",
        "        out.append(\"[BEGIN STORY]\")\n",
        "        out.append(sent['title'])\n",
        "        out.append(\"\")\n",
        "        last_title = sent['title']\n",
        "    out.append(sent['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VueTtu-LIJLO"
      },
      "source": [
        "Here's what the data look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBtnFezUIJLO",
        "outputId": "c2f229f1-73fe-492a-d021-38fa627e1cb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[BEGIN STORY]',\n",
              " 'Four Weddings and a Funeral',\n",
              " '',\n",
              " 'The film follows the adventures of a group of friends through the eyes of Charles, a good-natured but socially awkward man living in London, who becomes smitten with Carrie, an American whom Charles keeps meeting at four weddings and a funeral.',\n",
              " 'The first wedding is that of Angus and Laura, at which Charles is the best man.',\n",
              " 'Charles and his single friends wonder whether they will ever get married.',\n",
              " 'Charles meets Carrie and spends the night with her.',\n",
              " 'Carrie pretends that, now they have slept together, they will have to get married, to which Charles endeavours to respond before realising she is joking.',\n",
              " 'Carrie observes that they may have missed an opportunity and then returns to America.',\n",
              " 'The second wedding is that of Bernard and Lydia, a couple who became romantically involved at the previous wedding.',\n",
              " 'Charles encounters Carrie again, but she introduces him to her fiancé, Sir Hamish Banks, a wealthy politician.',\n",
              " 'At the reception, Charles finds himself seated with several ex-girlfriends who relate embarrassing stories about his inability to be discreet and afterwards bumps into Henrietta, known among Charles\\' friends as \"Duckface\", with whom he had a particularly difficult relationship.',\n",
              " 'Charles retreats to an empty hotel suite, seeing Carrie and Hamish leave in a taxicab, only to be trapped in a cupboard after the newlyweds stumble into the room to have sex.',\n",
              " 'After Charles awkwardly exits the room, Henrietta confronts him about his habit of \"serial monogamy\", telling him he is afraid of letting anyone get too close to him.',\n",
              " 'Charles then runs into Carrie, and they end up spending another night together.',\n",
              " \"A month later, Charles receives an invitation to Carrie's wedding.\",\n",
              " 'While shopping for a present, he coincidentally encounters Carrie and ends up helping her select her wedding dress.',\n",
              " 'Carrie lists her more than thirty sexual partners.',\n",
              " 'Charles later awkwardly tries confessing his love to her and hinting that he would like to have a relationship with her, to no avail.',\n",
              " 'The third wedding is that of Carrie and Hamish.',\n",
              " 'Charles attends, depressed at the prospect of Carrie marrying Hamish.',\n",
              " \"At the reception, Gareth instructs his friends to seek potential mates; Fiona's brother, Tom, stumbles through an attempt to connect with a woman until she reveals that she is the minister's wife, while Charles's flatmate, Scarlett, strikes up a conversation with an American named Chester.\",\n",
              " 'As Charles watches Carrie and Hamish dance, Fiona deduces his feelings about Carrie.',\n",
              " 'When Charles asks why Fiona is not married, she confesses that she has loved Charles since they first met years earlier.']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "out[:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI6WztFEIJLP"
      },
      "source": [
        "The following cell writes this out to a file, which you can then upload to the aitextgen notebook on Google Colab to train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAMoca5aIJLP"
      },
      "outputs": [],
      "source": [
        "with open(\"story_training.txt\", \"w\") as fh:\n",
        "    fh.write(\"\\n\".join(out))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijv0WkQWIJLP"
      },
      "source": [
        "In the text generation section of that notebook, try prompting the model with `[BEGIN STORY]` followed by the title of a story you'd like to generate!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}